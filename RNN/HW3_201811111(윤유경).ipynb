{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_201811111(Ïú§Ïú†Í≤Ω).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFuMIMZ0hy92",
        "outputId": "5c9e6fcd-d6ff-4fc2-c32a-e9c7e6068f75"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HB6V4mkh-FM",
        "outputId": "25ef1e5c-ee29-4658-ec69-1a385373b97b"
      },
      "source": [
        "import os, sys \n",
        "from google.colab import drive \n",
        "drive.mount('/content/mnt')\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/mnt; to attempt to forcibly remount, call drive.mount(\"/content/mnt\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "66f45kcph_zN",
        "outputId": "08eef220-0870-406a-f045-4c97ff6570b6"
      },
      "source": [
        "#data loader\n",
        "import csv\n",
        "import numpy as np\n",
        "import emoji\n",
        "import pandas as pd\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "\n",
        "def read_csv(filename = '/content/mnt/My Drive/ugrp/emojify_data.csv'):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
        "                    \"1\": \":baseball:\",\n",
        "                    \"2\": \":smile:\",\n",
        "                    \"3\": \":disappointed:\",\n",
        "                    \"4\": \":fork_and_knife:\"}\n",
        "\n",
        "def label_to_emoji(label):\n",
        "    \"\"\"\n",
        "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
        "    \"\"\"\n",
        "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
        "              \n",
        "    \n",
        "def print_predictions(X, pred):\n",
        "    print()\n",
        "    for i in range(X.shape[0]):\n",
        "        print(X[i], label_to_emoji(int(pred[i])))\n",
        "\n",
        "words_to_index, index_to_words, word_to_vec_map = read_glove_vecs('/content/mnt/My Drive/ugrp/glove.6B.50d.txt')\n",
        "X, Y = read_csv()\n",
        "label_to_emoji(Y[0])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'üç¥'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcRLjMqNiLVZ",
        "outputId": "429d8b9e-3fc2-4016-dbd5-a9e2ac01aa42"
      },
      "source": [
        "def to_one_hot(Y):\n",
        "    b = np.zeros((Y.size, Y.max()+1),dtype='i')\n",
        "    b[np.arange(Y.size),Y] = 1\n",
        "    return b\n",
        "\n",
        "print(to_one_hot(Y)[1])\n",
        "print(Y[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 1 0]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0g5lOgTiMzl",
        "outputId": "bbc5a8fd-5651-40e0-b72f-8cb409f0aec9"
      },
      "source": [
        "X_train_seq, Y_train = read_csv('/content/mnt/My Drive/ugrp/train_emoji.csv')\n",
        "X_test_seq, Y_test = read_csv('/content/mnt/My Drive/ugrp/test_emoji.csv')\n",
        "# X_test, Y_test = read_csv('data/tesss.csv')\n",
        "print('X_train: %i, Y_train: %i, X_test: %i, Y_test: %i' % (len(X_train_seq),len(Y_train),len(X_test_seq),len(Y_test)))\n",
        "Max_len = len(max(X_train_seq, key=len).split())\n",
        "print(Max_len)\n",
        "\n",
        "Y_train = to_one_hot(Y_train)\n",
        "Y_test = to_one_hot(Y_test)\n",
        "def sent2idx(X, words_to_index, max_len):\n",
        "    global word_to_vec_map\n",
        "    X_idx = np.zeros((X.shape[0] ,max_len),dtype='i')\n",
        "    for i in range(X.shape[0]):                              \n",
        "        sents =[wrd.lower() for wrd in X[i].split(\" \") if wrd.lower() in word_to_vec_map]\n",
        "        for wrd in range(len(sents)):\n",
        "            X_idx[i, wrd] = words_to_index[sents[wrd]]\n",
        "    return X_idx\n",
        "\n",
        "X_train = sent2idx(X_train_seq,words_to_index,len(max(X_train_seq, key=len).split()))\n",
        "X_test = sent2idx(X_test_seq,words_to_index,len(max(X_test_seq, key=len).split()))\n",
        "print('X:train: ',X_train)\n",
        "\n",
        "import random\n",
        "def split_validation(X_train, n=0.2):\n",
        "  #X_train = random.shuffle(X_train)\n",
        "  num = int(X_train.shape[0] * n)\n",
        "  print(num)\n",
        "  return X_train[num:], X_train[:num], Y_train[num:], Y_train[:num] #train, vallidation\n",
        "\n",
        "X_train1, X_train_valid, Y_train1, Y_train_valid = split_validation(X_train,n=0.2)\n",
        "print(X_train1.shape, X_train_valid.shape)\n",
        "print(Y_train1.shape, Y_train_valid.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: 132, Y_train: 132, X_test: 56, Y_test: 56\n",
            "10\n",
            "X:train:  [[259914 352214 360915 ...      0      0      0]\n",
            " [185457  52943 293982 ...      0      0      0]\n",
            " [193716 192973 357266 ... 222138      0      0]\n",
            " ...\n",
            " [386307 192973 390470 ...      0      0      0]\n",
            " [185457 226278 394475 ...      0      0      0]\n",
            " [166369 198213      0 ...      0      0      0]]\n",
            "26\n",
            "(106, 10) (26, 10)\n",
            "(106, 5) (26, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1v7Cy8ziOgx"
      },
      "source": [
        "class SGD:\n",
        "  def __init__(self, lr = 0.1):\n",
        "    self.lr = lr\n",
        "  def update(self, weights, gradients):\n",
        "    for w in range(len(weights)):\n",
        "      weights[w] -= self.lr * gradients[w]\n",
        "    return weights[w]\n",
        "    '''for w in weights:\n",
        "      weights[w] -= self.lr * gradients[w]\n",
        "    return weights[w]'''"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIP_XKmWiY6i"
      },
      "source": [
        "#referrence: https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/common/util.py\n",
        "def make_emb(emb_dim):   \n",
        "    global word_to_vec_map, words_to_index \n",
        "    vocab = len(words_to_index) + 1\n",
        "    emb = np.zeros((vocab, emb_dim))\n",
        "    for wrd, idx in words_to_index.items():\n",
        "        emb[idx, :] = word_to_vec_map[wrd]    \n",
        "    return emb\n",
        "\n",
        "\n",
        "def revise_grad(grads, max_grad=0.25):\n",
        "    #print('clip_before: ',grads)\n",
        "    summing = 0\n",
        "    for grad in grads:\n",
        "        summing += np.sum(grad ** 2)\n",
        "    rev = max_grad / (np.sqrt(summing) + 1e-6) \n",
        "    if rev < 1:#max_gradÎ≥¥Îã§ gradientÍ∞Ä Îçî ÌÅ¨Î©¥\n",
        "        for grad in grads:\n",
        "            grad *= rev\n",
        "    #print('clip_after: ',grads)\n",
        "    return grads\n",
        "\n",
        "def collect_parameters(weights, grads):\n",
        "    weights, grads = weights[:], grads[:]\n",
        "    while True:\n",
        "        k = False\n",
        "        for i in range(0, len(weights) - 1):\n",
        "            for j in range(i + 1, len(weights)):\n",
        "                if weights[i].ndim == 2 and weights[j].ndim == 2:\n",
        "                  if weights[j].shape==weights[i].T.shape and np.all(weights[i].T == weights[j]):\n",
        "                    grads[i] += grads[j].T\n",
        "                    k = True\n",
        "                    weights.pop(j)\n",
        "                    grads.pop(j)\n",
        "                if k: break\n",
        "            if k: break\n",
        "        if not k: break\n",
        "\n",
        "    return weights, grads\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Z6gNmxiadj"
      },
      "source": [
        "#Reference: https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/common/time_layers.py\n",
        "import numpy as np\n",
        "class vanilla_RNN_unit:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.Wx = Wx\n",
        "        self.Wh = Wh\n",
        "        self.b = b #tupleÎ°ú ÌïòÎ©¥ Í∞í update Î∂àÍ∞Ä\n",
        "\n",
        "        self.weights = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.x, self.h_inp, self.h_oup = None,None,None\n",
        "\n",
        "    def forward(self, x,h_inp):\n",
        "        #print('vanilla_RNN_u forward')\n",
        "        #print('h_inp: ',h_inp.shape,'self.Wh: ',self.Wh.shape,'self.Wx: ',self.Wx.shape)\n",
        "        #print('x: ',x.shape, 'self.b: ',self.b.shape)\n",
        "        '''h_inp:  (10, 128) self.Wh:  (128, 128) self.Wx:  (50, 128)\n",
        "            x:  (10, 50) self.b:  (128,)'''\n",
        "        h_oup = np.dot(h_inp, self.weights[1]) + np.dot(x, self.weights[0]) + self.weights[2]\n",
        "        h_oup = np.tanh(h_oup)\n",
        "        self.x =x\n",
        "        self.h_inp = h_inp\n",
        "        self.h_oup = h_oup\n",
        "        return h_oup\n",
        "\n",
        "    def backward(self, dh_oup):\n",
        "        doutp = dh_oup * (1 - self.h_oup ** 2)\n",
        "        db = np.sum(doutp, axis=0)\n",
        "        dWh = np.dot(self.h_inp.T, doutp)\n",
        "        dh_inp = np.dot(doutp, self.weights[1].T)\n",
        "        dWx = np.dot(self.x.T, doutp)\n",
        "        dx = np.dot(doutp, self.weights[0].T)\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_inp\n",
        "\n",
        "\n",
        "class vanilla_RNN:\n",
        "    def __init__(self, Wx, Wh, b, connect=False, out_seq = True):\n",
        "        self.weights = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.out_seq = out_seq\n",
        "        self.h, self.dh,self.hs,self.hidden, self.layers = None, None, None, None, None\n",
        "        self.connect = connect\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        self.layers = []\n",
        "        hs = np.empty((batch_x.shape[0], batch_x.shape[1], self.weights[0].shape[1]), dtype='f')\n",
        "        if not self.connect or self.h is None:\n",
        "            self.h = np.zeros((batch_x.shape[0], self.weights[0].shape[1]), dtype='f')\n",
        "        for t in range(batch_x.shape[1]):\n",
        "            layer = vanilla_RNN_unit(self.weights[0],self.weights[1],self.weights[2])\n",
        "            self.h = layer.forward(batch_x[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "        self.hidden = hs\n",
        "        if self.out_seq:\n",
        "          return hs\n",
        "        else:\n",
        "          self.hs = hs\n",
        "          return hs[:,-1,:]\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        dxs = np.empty((self.hidden.shape[0], self.hidden.shape[1], self.weights[0].shape[0]), dtype='f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        if not self.out_seq:\n",
        "          #print('LSTM out_seq=False backward')\n",
        "          dh = dhs+dh\n",
        "          for t in reversed(range(self.hidden.shape[1])):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dh)\n",
        "            dxs[:, t, :] = dx\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "        else:\n",
        "          for t in reversed(range(self.hidden.shape[1])):\n",
        "              layer = self.layers[t]\n",
        "              dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "              dxs[:, t, :] = dx\n",
        "              for i, grad in enumerate(layer.grads):\n",
        "                  grads[i] += grad\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "class LSTM_unit:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.weights = [Wx, Wh, b] #tupleÎ°ú ÌïòÎ©¥ Í∞í update X\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.x, self.h_inp, self.c_inp, self.i, self.f, self.g, self.o, self.c_out = None, None, None, None, None, None, None, None\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        #print('LSTM forward')\n",
        "        #print('input: ',x.shape,'h_prev: ',h_prev.shape,'c_prev: ',c_prev.shape)\n",
        "        num_batch, hidden = h.shape\n",
        "        #print(h_prev.shape, Wx.shape,Wh.shape,b.shape) #(50, 128) (128, 128) (128,)\n",
        "        lstm_ = np.dot(x, self.weights[0]) + np.dot(h, self.weights[1]) + self.weights[2]\n",
        "        f = lstm_[:, :hidden]\n",
        "        g = lstm_[:, hidden:2*hidden]\n",
        "        i = lstm_[:, 2*hidden:3*hidden]\n",
        "        o = lstm_[:, 3*hidden:]\n",
        "        #print('f,g,i,o: ',f,g,i,o)\n",
        "\n",
        "        f = 1 / (1 + np.exp(-f))\n",
        "        g = np.tanh(g)\n",
        "        i = 1 / (1 + np.exp(-i))\n",
        "        o = 1 / (1 + np.exp(-o))\n",
        "        #print('f: ',f.shape,'c_prev: ',c_prev.shape,'g: ',g,'i: ',i)\n",
        "        '''input x:  (21, 50) h_prev:  (21, 128) c_prev:  (21, 128)\n",
        "            A:  (21, 128)\n",
        "            f:  (21, 128) c_prev:  (21, 128) g:  (21, 0) i:  (21, 0)'''\n",
        "        c_out = f * c + g * i\n",
        "        h_out = o * np.tanh(c_out)\n",
        "        self.x = x\n",
        "        self.h_inp = h\n",
        "        self.c_inp = c\n",
        "        self.i = i\n",
        "        self.f = f\n",
        "        self.g = g\n",
        "        self.o = o\n",
        "        self.c_out = c_out\n",
        "        return h_out, c_out\n",
        "\n",
        "    def backward(self, dh, dc):\n",
        "        #print('LSTM backward')\n",
        "        x, h_inp, c_inp, i, f, g, o, c_out = self.x, self.h_inp, self.c_inp, self.i, self.f, self.g, self.o, self.c_out\n",
        "        ds = dc + (dh * o) * (1 - np.tanh(c_out) ** 2)\n",
        "        #print('ds',ds)\n",
        "        dc_inp = ds * f\n",
        "        #print('dc_prev: ',dc_prev)\n",
        "        di = ds * g\n",
        "        df = ds * c_inp\n",
        "        do = dh * np.tanh(c_out)\n",
        "        dg = ds * i\n",
        "        #print('di,df,do,dg: ',di,df,do,dg)\n",
        "        di *= i * (1 - i)\n",
        "        df *= f * (1 - f)\n",
        "        do *= o * (1 - o)\n",
        "        dg *= (1 - g ** 2)\n",
        "        dlstm_back = np.hstack((df, dg, di, do))\n",
        "        dWh = np.dot(h_inp.T, dlstm_back)\n",
        "        dWx = np.dot(x.T, dlstm_back)\n",
        "        db = dlstm_back.sum(axis=0)\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "        dx = np.dot(dlstm_back, self.weights[0].T)\n",
        "        dh_inp = np.dot(dlstm_back, self.weights[1].T)\n",
        "\n",
        "        return dx, dh_inp, dc_inp\n",
        "\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, Wx, Wh, b, connect=False, out_seq = True):\n",
        "        #print('LSTM init')\n",
        "        self.weights = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.h, self.c,self.dh,self.hidden,self.layers = None, None,None,None,None\n",
        "        self.connect = connect\n",
        "        self.out_seq = out_seq\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        #print('LSTM forward')\n",
        "        #print('LSTM, N,T,D: ',N,T,D)\n",
        "        #print('self.h: ',self.h)\n",
        "        hs = np.empty((batch_x.shape[0], batch_x.shape[1],self.weights[1].shape[0]), dtype='f')\n",
        "        if not self.connect or self.h is None:\n",
        "            #print('not statefulÏóê Îì§Ïñ¥Ïò¥')\n",
        "            self.h = np.zeros((batch_x.shape[0], self.weights[1].shape[0]), dtype='f')\n",
        "        if not self.connect or self.c is None:\n",
        "            self.c = np.zeros((batch_x.shape[0], self.weights[1].shape[0]), dtype='f')\n",
        "        #print('Wx,Wh,b,N,T,D,H,self.h: ',Wx.shape,Wh.shape,b.shape,N,T,D,H,self.h.shape)\n",
        "        self.layers = []\n",
        "        for t in range(batch_x.shape[1]):\n",
        "            layer = LSTM_unit(*self.weights)\n",
        "            self.h, self.c = layer.forward(batch_x[:, t, :], self.h, self.c)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "        self.hidden = hs\n",
        "        if self.out_seq: #True\n",
        "          #print('out: ',hs.shape)\n",
        "          return hs\n",
        "        #print('out: ',hs[:,-1,:].shape)\n",
        "        return hs[:,-1,:]\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        #print('LSTM backward')\n",
        "        dxs = np.empty((self.hidden.shape[0], self.hidden.shape[1], self.weights[0].shape[0]), dtype='f')\n",
        "        grads = [0, 0, 0]\n",
        "        dh, dc = 0, 0\n",
        "        if not self.out_seq:\n",
        "          #print('LSTM out_seq=False backward')\n",
        "          dh = dhs+dh\n",
        "          for t in reversed(range(self.hidden.shape[1])):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh, dc = layer.backward(dh, dc)\n",
        "            dxs[:, t, :] = dx\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "        else:\n",
        "          for t in reversed(range(self.hidden.shape[1])):\n",
        "              layer = self.layers[t]\n",
        "              dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
        "              dxs[:, t, :] = dx\n",
        "              for i, grad in enumerate(layer.grads):\n",
        "                  grads[i] += grad\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "        #print('out: ',dxs.shape)\n",
        "        #print('LSTM out: ',dxs)\n",
        "        return dxs\n",
        "class Dropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        #print('TimeDropout init')\n",
        "        self.weights, self.grads = [], []\n",
        "        self.d = dropout_ratio\n",
        "        self.m = None\n",
        "        self.train = True\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        #print('TimeDropout forward')\n",
        "        if self.train:\n",
        "            self.m = (np.random.rand(*batch_x.shape) > self.d).astype(np.float32) *(1 / (1.0 - self.d))\n",
        "            #print('out: ',(xs*self.m).shape)\n",
        "            return batch_x * self.m\n",
        "        #print('out: ',batch_x.shape)\n",
        "        return batch_x\n",
        "\n",
        "    def backward(self, back):\n",
        "        #print('out: ',(dout*self.m).shape)\n",
        "        return back * self.m"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtoqy9E1Wrqk"
      },
      "source": [
        "'''class Linear: \n",
        "  def __init__(self, w, b):\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.dw = 0\n",
        "    self.db = 0\n",
        "    self.x = 0\n",
        "    self.input_shape = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('Linear.forward')\n",
        "    #print('Linear.forward.x: ',x.shape)\n",
        "    #print('Linear.forward.w: ',self.W.shape)\n",
        "    self.input_shape = x.shape\n",
        "    self.x = x.reshape(x.shape[0], -1)\n",
        "    #print('after: ',x.shape)\n",
        "    #print('self.b: ',self.b.shape)\n",
        "\n",
        "    return np.dot(self.x, self.w) + self.b\n",
        "\n",
        "  def backward(self, back):\n",
        "    #print('Linear.backward')\n",
        "    self.dw = np.dot(self.x.T, back)\n",
        "    self.db = np.sum(back, axis=0)\n",
        "    return np.dot(back, self.w.T).reshape(*self.input_shape)'''\n",
        "\n",
        "class Linear:\n",
        "  def __init__(self, W, b):\n",
        "    #print('Linear init')        \n",
        "    self.x = None\n",
        "    self.original_x_shape = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "    self.weights = [W, b]\n",
        "    self.grads=[np.zeros_like(self.weights[0]),np.zeros_like(self.weights[1])]\n",
        "    self.original_x_shape = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('Linear.forward')\n",
        "    #print('Linear.forward.x: ',x.shape)\n",
        "    #print('Linear.forward.w: ',self.weights[0].shape)\n",
        "    self.original_x_shape = x.shape\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    #print('after: ',x.shape)\n",
        "    #print('self.b: ',self.b.shape)\n",
        "    self.x = x\n",
        "\n",
        "    out = np.dot(self.x, self.weights[0]) + self.weights[1]\n",
        "    #print('output: ',out.shape)\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    #print('Linear.backward')\n",
        "    iis = np.isnan(dout)\n",
        "    if True in iis:\n",
        "      print('Linear Nan here')\n",
        "    dx = np.dot(dout, self.weights[0].T)\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    #print('self.grads: ',self.grads)\n",
        "    self.grads[0][...] = self.dW\n",
        "    self.grads[1][...] = self.db\n",
        "    dx = dx.reshape(*self.original_x_shape)\n",
        "    #print('output: ',dx.shape)\n",
        "    return dx\n",
        "\n",
        "def softmax(x):\n",
        "  #print('_softmax')\n",
        "  '''#if x.ndim == 2:\n",
        "  x = x.T\n",
        "  x = x - np.max(x, axis=0)\n",
        "  y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "  return y.T'''\n",
        "  if x.ndim == 2:\n",
        "    x = x.T\n",
        "    x = x - np.max(x, axis=0)\n",
        "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "    return y.T \n",
        "\n",
        "  x = x - np.max(x) # Ïò§Î≤ÑÌîåÎ°ú ÎåÄÏ±Ö\n",
        "  return np.exp(x) / np.sum(np.exp(x))\n",
        "def cross_entropy_error(p,y):\n",
        "  #cross_entropy_error.p:  (100, 40, 13, 10)\n",
        "  #cross_entropy_error.y:  (100, 10)\n",
        "  #print('cross_entropy_error')\n",
        "  #print('cross_entropy_error.y: ',y.shape)\n",
        "  if p.ndim == 1:\n",
        "      y = y.reshape(1, y.size)\n",
        "      p = p.reshape(1, p.size)\n",
        "  #print('y: ',y)\n",
        "  if y.size == p.size:\n",
        "      y = y.argmax(axis=1) #dim ==1\n",
        "  #print('y: ',y)\n",
        "             \n",
        "  batch_size = p.shape[0]\n",
        "  #print('cross_entropy_error.p: ',p.shape)\n",
        "  #print('cross_entropy_error.y: ',y.shape)\n",
        "  #print('p[np.arange(batch_size), y]: ',p[np.arange(batch_size), y])\n",
        "  return -np.sum(np.log(p[np.arange(batch_size), y] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "class Softmax_Cross_Entropy_Error:\n",
        "  def __init__(self):\n",
        "    self.loss = 0\n",
        "    self.p = 0\n",
        "    self.y = 0\n",
        "\n",
        "  def forward(self, p, y):\n",
        "    #print('Softmax_Cross_Entropy_Error_forward')\n",
        "    self.y = y\n",
        "    self.p = softmax(p)\n",
        "    loss = cross_entropy_error(self.p, self.y)\n",
        "    #print('out: ',loss)\n",
        "    self.loss = loss\n",
        "    return loss\n",
        "\n",
        "  def backward(self, back = 1):\n",
        "    #print('Softmax_Cross_Entropy_Error_backward')\n",
        "    batch_size = self.y.shape[0]\n",
        "    return (self.p - self.y) / batch_size\n",
        "\n",
        "\n",
        "class Embedding_unit:\n",
        "    def __init__(self, W):\n",
        "      #print('Embedding_unit init')\n",
        "      self.weights = [W] #(vocab_size, emb_size)\n",
        "      self.grads =  [np.zeros_like(W)]\n",
        "      self.wrd_idx = None\n",
        "\n",
        "    def forward(self, wrd_idx):\n",
        "      #print('Embedding_unit forward')\n",
        "      self.wrd_idx = wrd_idx\n",
        "      out = self.weights[0][wrd_idx]\n",
        "      return out\n",
        "      '''#print('Embedding_unit forward')\n",
        "      #print('word_idx: ',word_idx)\n",
        "        #print('weight: ',self.weight)\n",
        "      l = word_idx.shape\n",
        "      word_vec = self.weights[0][word_idx]\n",
        "      #print('word_vec: ',word_vec.shape)\n",
        "      #assert (word_vec.shape == (self.weight.shape[1],l))\n",
        "      self.word_idx = word_idx\n",
        "      #print('word_idx: ',word_idx)\n",
        "      return word_vec\n",
        "      #out = self.weight[word_idx]\n",
        "      #self.word_idx = word_idx'''\n",
        "\n",
        "    def backward(self, dback):\n",
        "      #print('Embedding_unit backward')\n",
        "      #print('dback: ',dback)\n",
        "      self.grads[0][...] = 0\n",
        "      #print('dw: ',dW.shape)\n",
        "      #print('word_idx: ',self.word_idx.shape)\n",
        "      #print('dback: ',dback.shape)\n",
        "      #print('db: ',dback)\n",
        "      np.add.at(self.grads[0], self.wrd_idx, dback) #np.add.at(A, idx, B)Îäî BÎ•º AÏùò idx Î≤àÏß∏ ÌñâÏóê ÎçîÌïòÎäî Ïó∞ÏÇ∞\n",
        "      #print('emb_unit_grads: ',self.grads)\n",
        "      return None\n",
        "\n",
        "class Embedding:\n",
        "    def __init__(self, vocab_size, emb_size,emb_pre = True):\n",
        "        #print('Embedding init')\n",
        "        global word_to_vec_map, words_to_index\n",
        "        if emb_pre is True:\n",
        "          self.emb_w = make_emb(emb_size)\n",
        "        else:\n",
        "          self.emb_w = (0.01* np.random.randn(vocab_size, emb_size)).astype('f')\n",
        "        self.weights = [self.emb_w]\n",
        "        self.grads = [np.zeros_like(self.emb_w)]\n",
        "        self.W = self.emb_w\n",
        "        self.layers = None\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        #print('Embedding forward')\n",
        "        #print('N: ',N,'T: ',T,'D: ',D) #N:  6 T:  10 D:  50\n",
        "        out = np.empty((batch_x.shape[0], batch_x.shape[1], self.W.shape[1]), dtype='f')\n",
        "        self.layers = []\n",
        "        for t in range(batch_x.shape[1]):\n",
        "            layer = Embedding_unit(self.W)\n",
        "            out[:,t, :] = layer.forward(batch_x[:,t])\n",
        "            self.layers.append(layer)\n",
        "        #print('out: ',out.shape)\n",
        "        return out\n",
        "    def backward(self, back):\n",
        "        #print('Embedding backward')\n",
        "        iis = np.isnan(back)\n",
        "        if True in iis:\n",
        "          print('Embedding_Nan here')\n",
        "        grad = 0\n",
        "        for t in range(back.shape[1]):\n",
        "            layer = self.layers[t]\n",
        "            layer.backward(back[:, t, :])\n",
        "            grad += layer.grads[0]\n",
        "        #print('Final_Embedding_grad: ',grad)\n",
        "        self.grads[0][...] = grad\n",
        "        return None\n",
        "class ADAM:\n",
        "    def __init__(self, lr=0.001, b1=0.9, b2=0.999):\n",
        "        self.lr = lr\n",
        "        self.b = [b1,b2]\n",
        "        self.i = 0\n",
        "        self.alpha = None\n",
        "        self.bet = None\n",
        "        \n",
        "    def update(self, weights, grads):\n",
        "        if self.alpha is None:\n",
        "            self.alpha, self.bet = [], []\n",
        "            for w in weights:\n",
        "                self.alpha.append(np.zeros_like(w))\n",
        "                self.bet.append(np.zeros_like(w))\n",
        "        \n",
        "        self.i += 1\n",
        "        _adam = self.lr * np.sqrt(1.0 - self.b[1]**self.i) / (1.0 - self.b[0]**self.i)\n",
        "\n",
        "        for i in range(len(weights)):\n",
        "            self.alpha[i] += (1 - self.b[0]) * (grads[i] - self.alpha[i])\n",
        "            self.bet[i] += (1 - self.b[1]) * (grads[i]**2 - self.bet[i])\n",
        "            weights[i] -= _adam * self.alpha[i] / (np.sqrt(self.bet[i]) + 1e-7)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl2uI7rgXTfJ"
      },
      "source": [
        "class LSTM_ADAM_Model():\n",
        "    def __init__(self,lr, vocab_size=400000, emb_size=50,hidden_size=128, dropout_ratio=0.5):\n",
        "        self.Wx = (np.random.randn(emb_size, 4*hidden_size) / np.sqrt(emb_size)).astype('f')\n",
        "        self.Wh = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\n",
        "        self.b = np.zeros(4*hidden_size).astype('f')\n",
        "        self.Wx_ = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\n",
        "        self.Wh_ = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\n",
        "        self.b_ = np.zeros(4*hidden_size).astype('f')\n",
        "        self.linear_w = np.zeros((hidden_size,5)).astype('f')\n",
        "        self.linear_b = np.zeros(5).astype('f')\n",
        "        self.optimizer = ADAM(lr)\n",
        "\n",
        "        self.layers = [\n",
        "            Embedding(vocab_size, emb_size,emb_pre=True),\n",
        "            LSTM(self.Wx, self.Wh, self.b, connect=True,out_seq=True),\n",
        "            #Dropout(dropout_ratio),\n",
        "            LSTM(self.Wx_, self.Wh_, self.b_, connect=True,out_seq = False),\n",
        "            #Dropout(dropout_ratio),\n",
        "            Linear(self.linear_w, self.linear_b)\n",
        "        ]\n",
        "        self.last_layer = Softmax_Cross_Entropy_Error()\n",
        "\n",
        "        self.weights, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.weights += layer.weights\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self,batch_x, batch_y, train=True):\n",
        "        if train:\n",
        "          if len(self.layers) != 4: #dropoutÏóÜÏñ¥\n",
        "            self.layers[2].train = train\n",
        "            self.layers[4].train = train\n",
        "          for layer in self.layers: #Dropout = TrainÎ≥¥ÎÇ¥\n",
        "              batch_x = layer.forward(batch_x)\n",
        "          score = batch_x\n",
        "          loss = self.last_layer.forward(score, batch_y)\n",
        "          return loss\n",
        "        else: #testÏãú\n",
        "          if len(self.layers) != 4: #dropoutÏóÜÏñ¥\n",
        "            self.layers[2].train = False\n",
        "            self.layers[4].train = False\n",
        "          for layer in self.layers:\n",
        "              batch_x = layer.forward(batch_x)\n",
        "          return batch_x\n",
        "\n",
        "    def back_propagation(self, back=1):\n",
        "        back = self.last_layer.backward(back)\n",
        "        for layer in reversed(self.layers):\n",
        "            back = layer.backward(back)\n",
        "        return back"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7op9JK7KYmY1"
      },
      "source": [
        "class LSTM_SGD_Model():\r\n",
        "    def __init__(self,lr, vocab_size=400000, emb_size=50,hidden_size=128, dropout_ratio=0.5):\r\n",
        "        self.Wx = (np.random.randn(emb_size, 4*hidden_size) / np.sqrt(emb_size)).astype('f')\r\n",
        "        self.Wh = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\r\n",
        "        self.b = np.zeros(4*hidden_size).astype('f')\r\n",
        "        self.Wx_ = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\r\n",
        "        self.Wh_ = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\r\n",
        "        self.b_ = np.zeros(4*hidden_size).astype('f')\r\n",
        "        self.linear_w = np.zeros((hidden_size,5)).astype('f')\r\n",
        "        self.linear_b = np.zeros(5).astype('f')\r\n",
        "        self.optimizer = SGD(lr)\r\n",
        "\r\n",
        "        self.layers = [\r\n",
        "            Embedding(vocab_size, emb_size,emb_pre=True),\r\n",
        "            LSTM(self.Wx, self.Wh, self.b, connect=True,out_seq=True),\r\n",
        "            #Dropout(dropout_ratio),\r\n",
        "            LSTM(self.Wx_, self.Wh_, self.b_, connect=True,out_seq = False),\r\n",
        "            #Dropout(dropout_ratio),\r\n",
        "            Linear(self.linear_w, self.linear_b)\r\n",
        "        ]\r\n",
        "        self.last_layer = Softmax_Cross_Entropy_Error()\r\n",
        "\r\n",
        "        self.weights, self.grads = [], []\r\n",
        "        for layer in self.layers: #updateÌïòÍ∏∞ ÏúÑÌï¥ÏÑú Í∞Å Î†àÏù¥Ïñ¥Ïùò weightÏôÄ gradÎ•º Í∞ùÏ≤¥Î°ú Î™®ÏúºÍ∏∞\r\n",
        "            self.weights += layer.weights\r\n",
        "            self.grads += layer.grads\r\n",
        "\r\n",
        "    def forward(self,batch_x, batch_y, train=True):\r\n",
        "        if train:\r\n",
        "          if len(self.layers) != 4: #dropoutÏóÜÏñ¥\r\n",
        "            self.layers[2].train = train\r\n",
        "            self.layers[4].train = train\r\n",
        "          for layer in self.layers: #Dropout = TrainÎ≥¥ÎÇ¥\r\n",
        "              batch_x = layer.forward(batch_x)\r\n",
        "          score = batch_x\r\n",
        "          loss = self.last_layer.forward(score, batch_y)\r\n",
        "          return loss\r\n",
        "        else: #testÏãú\r\n",
        "          if len(self.layers) != 4: #dropoutÏóÜÏñ¥\r\n",
        "            self.layers[2].train = False\r\n",
        "            self.layers[4].train = False\r\n",
        "          for layer in self.layers:\r\n",
        "              batch_x = layer.forward(batch_x)\r\n",
        "          return batch_x\r\n",
        "\r\n",
        "    def back_propagation(self, back=1):\r\n",
        "        back = self.last_layer.backward(back)\r\n",
        "        for layer in reversed(self.layers):\r\n",
        "            back = layer.backward(back)\r\n",
        "        return back"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpoysOsZY5aB"
      },
      "source": [
        "class LSTM_SGD_Drop_Model():\r\n",
        "    def __init__(self,lr, vocab_size=400000, emb_size=50,hidden_size=128, dropout_ratio=0.5):\r\n",
        "        self.Wx = (np.random.randn(emb_size, 4*hidden_size) / np.sqrt(emb_size)).astype('f')\r\n",
        "        self.Wh = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\r\n",
        "        self.b = np.zeros(4*hidden_size).astype('f')\r\n",
        "        self.Wx_ = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\r\n",
        "        self.Wh_ = (np.random.randn(hidden_size, 4*hidden_size) / np.sqrt(hidden_size)).astype('f')\r\n",
        "        self.b_ = np.zeros(4*hidden_size).astype('f')\r\n",
        "        self.linear_w = np.zeros((hidden_size,5)).astype('f')\r\n",
        "        self.linear_b = np.zeros(5).astype('f')\r\n",
        "        self.optimizer = SGD(lr)\r\n",
        "\r\n",
        "        self.layers = [\r\n",
        "            Embedding(vocab_size, emb_size,emb_pre=True),\r\n",
        "            LSTM(self.Wx, self.Wh, self.b, connect=True,out_seq=True),\r\n",
        "            Dropout(dropout_ratio),\r\n",
        "            LSTM(self.Wx_, self.Wh_, self.b_, connect=True,out_seq = False),\r\n",
        "            Dropout(dropout_ratio),\r\n",
        "            Linear(self.linear_w, self.linear_b)\r\n",
        "        ]\r\n",
        "        self.last_layer = Softmax_Cross_Entropy_Error()\r\n",
        "\r\n",
        "        self.weights, self.grads = [], []\r\n",
        "        for layer in self.layers: #updateÌïòÍ∏∞ ÏúÑÌï¥ÏÑú Í∞Å Î†àÏù¥Ïñ¥Ïùò weightÏôÄ gradÎ•º Í∞ùÏ≤¥Î°ú Î™®ÏúºÍ∏∞\r\n",
        "            self.weights += layer.weights\r\n",
        "            self.grads += layer.grads\r\n",
        "\r\n",
        "    def forward(self,batch_x, batch_y, train=True):\r\n",
        "        if train:\r\n",
        "          if len(self.layers) != 4: #dropoutÏóÜÏñ¥\r\n",
        "            self.layers[2].train = train\r\n",
        "            self.layers[4].train = train\r\n",
        "          for layer in self.layers: #Dropout = TrainÎ≥¥ÎÇ¥\r\n",
        "              batch_x = layer.forward(batch_x)\r\n",
        "          score = batch_x\r\n",
        "          loss = self.last_layer.forward(score, batch_y)\r\n",
        "          return loss\r\n",
        "        else: #testÏãú\r\n",
        "          if len(self.layers) != 4: #dropoutÏóÜÏñ¥\r\n",
        "            self.layers[2].train = False\r\n",
        "            self.layers[4].train = False\r\n",
        "          for layer in self.layers:\r\n",
        "              batch_x = layer.forward(batch_x)\r\n",
        "          return batch_x\r\n",
        "\r\n",
        "    def back_propagation(self, back=1):\r\n",
        "        back = self.last_layer.backward(back)\r\n",
        "        for layer in reversed(self.layers):\r\n",
        "            back = layer.backward(back)\r\n",
        "        return back"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YubHNUTiWmvq",
        "outputId": "0d7270db-2024-40dd-8358-18fbef40eb30"
      },
      "source": [
        "%%time\n",
        "\n",
        "mini_batch = 2\n",
        "lr = 0.7\n",
        "\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "v_loss = 0\n",
        "\n",
        "model = LSTM_SGD_Drop_Model(lr,emb_size = 50) #50dim\n",
        "idx = (len(X_train1) - 1) // mini_batch #131/6 = 21 #2Í∞úÏùò batch Î¨∂ÏùåÏùÑ ÎßåÎì§Í≤†Îã§!\n",
        "offsets = [i * idx for i in range(mini_batch)]\n",
        "v_x = np.concatenate((X_train_valid, X_train1[:idx-len(X_train_valid)]), axis=0)\n",
        "v_y = np.concatenate((Y_train_valid, Y_train1[:idx-len(X_train_valid)]), axis=0)\n",
        "print('validation_x: ',v_x.shape,'validation_y: ',v_y.shape)\n",
        "for epoch in range(20): #epoch Ïàò\n",
        "    print('epoch: ',epoch)\n",
        "    total_loss = 0\n",
        "    for i, offset in enumerate(offsets):\n",
        "      #print(i,offset, X_train1[offset:offset+idx+1].shape)\n",
        "      batch_x = X_train1[offset:offset+idx]\n",
        "      batch_trg = Y_train1[offset:offset+idx]    \n",
        "      #print('batch_x: ',batch_x,'batch_trg: ',batch_trg)\n",
        "      #print('before_weights: ',model.weights[0][0])\n",
        "      loss = model.forward(batch_x, batch_trg)\n",
        "      total_loss +=loss\n",
        "      print('Loss=--------------: ',loss)\n",
        "      model.back_propagation()\n",
        "      weights,grads = collect_parameters(model.weights, model.grads)\n",
        "      #print('weights: ',len(weights),'grads: ',len(grads)) #(9,9)\n",
        "      #for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        #np.clip(dparam, -5, 5, out=dparam)\n",
        "        #weights,grads = model.weights, model.grads\n",
        "      grads=revise_grad(grads, max_grad=0.7) \n",
        "      model.optimizer.update(weights, grads)\n",
        "      #print('after_weights: ',model.weights[0][0])\n",
        "    v = model.forward(v_x,v_y,train=False) #pred\n",
        "    v_loss = model.last_layer.forward(v[:26],v_y[:26])\n",
        "    print('v_loss: ',v_loss)\n",
        "    valid_loss.append(v_loss)\n",
        "    train_loss.append(total_loss / len(offsets)) #Ï¥ù Î∞∞Ïπò Ïàò"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation_x:  (52, 10) validation_y:  (52, 5)\n",
            "epoch:  0\n",
            "Loss=--------------:  1.6094372089092548\n",
            "Loss=--------------:  1.6025477189284105\n",
            "v_loss:  1.5653721736027644\n",
            "epoch:  1\n",
            "Loss=--------------:  1.585380114041842\n",
            "Loss=--------------:  1.5911727318396935\n",
            "v_loss:  1.5371839083158052\n",
            "epoch:  2\n",
            "Loss=--------------:  1.5722540341890776\n",
            "Loss=--------------:  1.587131353525015\n",
            "v_loss:  1.520013222327599\n",
            "epoch:  3\n",
            "Loss=--------------:  1.5633697509765625\n",
            "Loss=--------------:  1.5813258244441106\n",
            "v_loss:  1.506598399235652\n",
            "epoch:  4\n",
            "Loss=--------------:  1.5534654764028697\n",
            "Loss=--------------:  1.577284886286809\n",
            "v_loss:  1.4953016134408803\n",
            "epoch:  5\n",
            "Loss=--------------:  1.5529826237605169\n",
            "Loss=--------------:  1.5779939798208384\n",
            "v_loss:  1.4909908588115985\n",
            "epoch:  6\n",
            "Loss=--------------:  1.5503653012789214\n",
            "Loss=--------------:  1.5740980001596303\n",
            "v_loss:  1.4855851393479567\n",
            "epoch:  7\n",
            "Loss=--------------:  1.5425369556133564\n",
            "Loss=--------------:  1.5732581798846905\n",
            "v_loss:  1.479680868295523\n",
            "epoch:  8\n",
            "Loss=--------------:  1.5370857532207782\n",
            "Loss=--------------:  1.5656615037184496\n",
            "v_loss:  1.4757919311523438\n",
            "epoch:  9\n",
            "Loss=--------------:  1.541985878577599\n",
            "Loss=--------------:  1.5638160705566406\n",
            "v_loss:  1.4717363210824819\n",
            "epoch:  10\n",
            "Loss=--------------:  1.530333005464994\n",
            "Loss=--------------:  1.5635785322922926\n",
            "v_loss:  1.4695678124060998\n",
            "epoch:  11\n",
            "Loss=--------------:  1.5272119962252104\n",
            "Loss=--------------:  1.5558160635141225\n",
            "v_loss:  1.4661234342134917\n",
            "epoch:  12\n",
            "Loss=--------------:  1.5204293177678034\n",
            "Loss=--------------:  1.5412411322960486\n",
            "v_loss:  1.4592546316293569\n",
            "epoch:  13\n",
            "Loss=--------------:  1.5182223686805139\n",
            "Loss=--------------:  1.530512442955604\n",
            "v_loss:  1.4503770974966197\n",
            "epoch:  14\n",
            "Loss=--------------:  1.5007529625525842\n",
            "Loss=--------------:  1.5099474099966197\n",
            "v_loss:  1.4356613159179688\n",
            "epoch:  15\n",
            "Loss=--------------:  1.490595450768104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n7NFS9xqn_9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.plot(valid_loss)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss','valid_loss'])\n",
        "plt.title('LSTM+ADAM+50d model Loss graph')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPtg5pz9Eg94"
      },
      "source": [
        "#test data accuracy\n",
        "print(X_test.shape)\n",
        "new_split_X_test = np.concatenate((X_test[52:], X_test[:48]), axis=0) #56+50=106\n",
        "print(new_split_X_test.shape)\n",
        "p = np.argmax(model.forward(X_test[:52],X_test[:52],train=False), axis = 1)  #batch_yÎäî Í∑∏ÎÉ• Î¨¥Ïãú\n",
        "p2 = np.argmax(model.forward(new_split_X_test,new_split_X_test,train=False), axis = 1)[:4]\n",
        "p = np.concatenate((p,p2), axis = 0)\n",
        "#one_hot encodingÏù¥ÎãàÍπå ifÎ¨∏ Ï†ÅÏö© x.!!\n",
        "y = np.argmax(Y_test, axis=1) #Ìñâ (1,batch_size)\n",
        "print(\"correct: \",np.sum(y == p))\n",
        "accuracy = np.sum(y == p) / 56 # Ï†ÑÏ≤¥Í∞úÏàò\n",
        "print(accuracy)\n",
        "print(len(p))\n",
        "for i in range(len(p)):\n",
        "  print(X_test_seq[i],label_to_emoji(p[i]),label_to_emoji(np.argmax(Y_test[i])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6WMaHbbIf-r"
      },
      "source": [
        "#RNNÏö©\r\n",
        "class RNN:\r\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size=128, emb_pre = True,dropout_ratio=0.5):\r\n",
        "        Wx = (np.random.randn(emb_size, hidden_size) / np.sqrt(emb_size)).astype('f')\r\n",
        "        Wh = (np.random.randn(hidden_size, hidden_size) / np.sqrt(hidden_size)).astype('f')\r\n",
        "        b = np.zeros(hidden_size).astype('f')\r\n",
        "        Wx_2 = (np.random.randn(hidden_size, hidden_size)/ np.sqrt(hidden_size)).astype('f')\r\n",
        "        Wh_2 = (np.random.randn(hidden_size, hidden_size)/ np.sqrt(hidden_size)).astype('f')\r\n",
        "        b_2 = np.zeros(hidden_size).astype('f')\r\n",
        "        linear_W = (np.random.randn(hidden_size, 5)).astype('f') #5Îäî Y_train.shape\r\n",
        "        linear_b = np.zeros(5).astype('f')\r\n",
        "\r\n",
        "        self.layers = [Embedding(vocab_size, emb_size,emb_pre), \r\n",
        "                       vanilla_RNN(Wx, Wh, b,out_seq=True,connect = True),\r\n",
        "                       #Dropout(dropout_ratio),\r\n",
        "                       vanilla_RNN(Wx_2, Wh_2, b_2,out_seq=False,connect = True),\r\n",
        "                       #Dropout(dropout_ratio),\r\n",
        "                       Linear(linear_W, linear_b)\r\n",
        "                       ]\r\n",
        "        self.last_layer = Softmax_Cross_Entropy_Error() \r\n",
        "        #updateÏúÑÌï¥ÏÑú\r\n",
        "        self.weights, self.grads = [], []\r\n",
        "        for layer in self.layers:\r\n",
        "            #print('layer.weights: ',len(layer.weights),'layer.grads: ',len(layer.grads))\r\n",
        "            self.weights+=layer.weights #appendÎûë Í∞ôÏùå\r\n",
        "            self.grads+=layer.grads\r\n",
        "        #print('model.weights: ',len(self.weights),'model.grads: ',len(self.grads))\r\n",
        "    \r\n",
        "    def forward(self,batch_x, batch_y, train=True):\r\n",
        "        if train:\r\n",
        "          if len(self.layers) != 4: #dropoutÏóÜÏñ¥\r\n",
        "            self.layers[2].train = train\r\n",
        "            self.layers[4].train = train\r\n",
        "          for layer in self.layers: #Dropout = TrainÎ≥¥ÎÇ¥\r\n",
        "              batch_x = layer.forward(batch_x)\r\n",
        "          score = batch_x\r\n",
        "          loss = self.last_layer.forward(score, batch_y)\r\n",
        "          return loss\r\n",
        "        else: #testÏãú\r\n",
        "          if len(self.layers) != 4: #dropoutÏóÜÏñ¥\r\n",
        "            self.layers[2].train = False\r\n",
        "            self.layers[4].train = False\r\n",
        "          for layer in self.layers:\r\n",
        "              batch_x = layer.forward(batch_x)\r\n",
        "          return batch_x\r\n",
        "\r\n",
        "    def back_propagation(self, back=1):\r\n",
        "        back = self.last_layer.backward(back)\r\n",
        "        for layer in reversed(self.layers):\r\n",
        "            back = layer.backward(back)\r\n",
        "        return back"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUrrluOxW9Ub"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "mini_batch = 2\r\n",
        "lr = 0.7\r\n",
        "\r\n",
        "total_loss = 0\r\n",
        "loss_count = 0\r\n",
        "train_loss = []\r\n",
        "valid_loss = []\r\n",
        "v_loss = 0\r\n",
        "\r\n",
        "model = RNN(len(word_to_vec_map), 50, 128) #50dim\r\n",
        "optimizer = SGD(lr)\r\n",
        "idx = (len(X_train1) - 1) // mini_batch #131/6 = 21 #2Í∞úÏùò batch Î¨∂ÏùåÏùÑ ÎßåÎì§Í≤†Îã§!\r\n",
        "print('idx: ',idx)\r\n",
        "offsets = [i * idx for i in range(mini_batch)]\r\n",
        "v_x = np.concatenate((X_train_valid, X_train1[:idx-len(X_train_valid)]), axis=0)\r\n",
        "v_y = np.concatenate((Y_train_valid, Y_train1[:idx-len(X_train_valid)]), axis=0)\r\n",
        "print('validation_x: ',v_x.shape,'validation_y: ',v_y.shape)\r\n",
        "offsets = [i * idx for i in range(mini_batch)]\r\n",
        "for epoch in range(20): #epoch Ïàò\r\n",
        "    print('epoch: ',epoch)\r\n",
        "    total_loss = 0\r\n",
        "    for i, offset in enumerate(offsets):\r\n",
        "      #print(i,offset, X_train1[offset:offset+idx+1].shape)\r\n",
        "      batch_x = X_train1[offset:offset+idx] #vocabÏúºÎ°ú Î∞îÍøî\r\n",
        "      batch_trg = Y_train1[offset:offset+idx]    \r\n",
        "      #print('batch_x: ',batch_x,'batch_trg: ',batch_trg)\r\n",
        "      #print('before_weights: ',model.weights[0][0])\r\n",
        "      loss = model.forward(batch_x, batch_trg)\r\n",
        "      total_loss +=loss\r\n",
        "      print('Loss=--------------: ',loss)\r\n",
        "      model.back_propagation()\r\n",
        "      weights,grads = collect_parameters(model.weights, model.grads)\r\n",
        "      #print('weights: ',len(weights),'grads: ',len(grads)) #(9,9)\r\n",
        "      #for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\r\n",
        "        #np.clip(dparam, -5, 5, out=dparam)\r\n",
        "        #weights,grads = model.weights, model.grads\r\n",
        "      grads=revise_grad(grads, max_grad=0.7)\r\n",
        "      optimizer.update(weights, grads)\r\n",
        "      #print('after_weights: ',model.weights[0][0])\r\n",
        "    v = model.forward(v_x,v_y,train=False)\r\n",
        "    v_loss = model.last_layer.forward(v[:26],v_y[:26])\r\n",
        "    print('v_loss: ',v_loss)\r\n",
        "    valid_loss.append(v_loss)\r\n",
        "    train_loss.append(total_loss / len(offsets)) #Ï¥ù Î∞∞Ïπò Ïàò"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvrt2fhvW_yT"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.plot(train_loss)\r\n",
        "plt.plot(valid_loss)\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train_loss','valid_loss'])\r\n",
        "plt.title('RNN+SGD+50d model Loss graph')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8l82Ep6XB4q"
      },
      "source": [
        "#test data accuracy\r\n",
        "print(X_test.shape)\r\n",
        "new_split_X_test = np.concatenate((X_test[52:], X_test[:48]), axis=0) #56+50=106\r\n",
        "print(new_split_X_test.shape)\r\n",
        "p = np.argmax(model.forward(X_test[:52],X_test[:52],train=False), axis = 1) #argmaxÎäî Í∞ÄÏû• ÌÅ∞ Í∞íÏùò Ïù∏Îç±Ïä§ Í∞íÏùÑ Î∞òÌôòÌïúÎã§.\r\n",
        "p2 = np.argmax(model.forward(new_split_X_test,new_split_X_test,train=False), axis = 1)[:4] #Ï≤òÏùå 4Í∞úÎßå\r\n",
        "p = np.concatenate((p,p2), axis = 0)\r\n",
        "#one_hot encodingÏù¥ÎãàÍπå ifÎ¨∏ Ï†ÅÏö© x.!!\r\n",
        "y = np.argmax(Y_test, axis=1) #Ìñâ (1,batch_size)\r\n",
        "print(\"correct: \",np.sum(y == p))\r\n",
        "accuracy = np.sum(y == p) / 56 # Ï†ÑÏ≤¥Í∞úÏàò\r\n",
        "print('accuracy: ',accuracy)\r\n",
        "print('test_set Í∞úÏàò:',len(p))\r\n",
        "for i in range(len(p)):\r\n",
        "  print(X_test_seq[i],label_to_emoji(p[i]),label_to_emoji(np.argmax(Y_test[i])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKlGJkiBB5mq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}